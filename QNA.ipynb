{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOmr75cWzpzx84lTw158wLC"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"aMDDafz0KcBX"},"outputs":[],"source":["import tensorflow as tf\n","import tensorflow_hub as hub\n","from transformers import BertTokenizer"]},{"cell_type":"code","source":["tokenizer = BertTokenizer.from_pretrained('tokenizer_tf2_qa')\n","model = hub.load(\"https://tfhub.dev/see--/bert-uncased-tf2-qa/1\")"],"metadata":{"id":"5fmRrcKaKlbW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["questions = [\n","    'How long did it take to find the answer?',\n","    'What\\'s the answer to the great question?',\n","    'What\\'s the name of the computer?']\n","paragraph = '''<p>The computer is named Deep Thought.</p>.\n","               <p>After 46 million years of training it found the answer.</p>\n","               <p>However, nobody was amazed. The answer was 42.</p>'''\n","\n","for question in questions:\n","  question_tokens = tokenizer.tokenize(question)\n","  paragraph_tokens = tokenizer.tokenize(paragraph)\n","  tokens = ['[CLS]'] + question_tokens + ['[SEP]'] + paragraph_tokens + ['[SEP]']\n","  input_word_ids = tokenizer.convert_tokens_to_ids(tokens)\n","  input_mask = [1] * len(input_word_ids)\n","  input_type_ids = [0] * (1 + len(question_tokens) + 1) + [1] * (len(paragraph_tokens) + 1)\n","\n","  input_word_ids, input_mask, input_type_ids = map(lambda t: tf.expand_dims(\n","      tf.convert_to_tensor(t, dtype=tf.int32), 0), (input_word_ids, input_mask, input_type_ids))\n","  outputs = model([input_word_ids, input_mask, input_type_ids])\n","  # using `[1:]` will enforce an answer. `outputs[0][0][0]` is the ignored '[CLS]' token logit\n","  short_start = tf.argmax(outputs[0][0][1:]) + 1\n","  short_end = tf.argmax(outputs[1][0][1:]) + 1\n","  answer_tokens = tokens[short_start: short_end + 1]\n","  answer = tokenizer.convert_tokens_to_string(answer_tokens)\n","  print(f'Question: {question}')\n","  print(f'Answer: {answer}')\n"],"metadata":{"id":"s8a3a5q0Kqwj"},"execution_count":null,"outputs":[]}]}